# åŸºç¡€ç¯å¢ƒçš„é…ç½®ï¼ˆåŸºäºWSLï¼‰
## åœ¨wslä¸‹è½½pytorch
æ‰“å¼€wsl

```bash
# ä¸‹è½½ Miniconda å®‰è£…è„šæœ¬
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
# è¿è¡Œå®‰è£…è„šæœ¬
bash Miniconda3-latest-Linux-x86_64.sh
# æŒ‰ç…§æç¤ºå®Œæˆå®‰è£…ï¼Œå¹¶é‡å¯ç»ˆç«¯
```
åƒä»¥å‰ä¸€æ ·é…ç½®æ–°ç¯å¢ƒï¼Œè£…pytorchï¼Œå®Œæˆåæ£€éªŒï¼š
```bash
python -c "import torch; print(f'PyTorch version: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'CUDA version used by PyTorch: {torch.version.cuda}')"
```
è¾“å‡º
```
PyTorch version: 2.8.0+cu128
CUDA available: True
CUDA version used by PyTorch: 12.8
```
å³ä¸ºæˆåŠŸ

**æ³¨æ„ï¼Œå¦‚æœè¦ä¸‹è½½vllmåŠ é€Ÿï¼Œè™šæ‹Ÿç¯å¢ƒçš„pythonåº”ä¸º3.9-3.12ï¼Œè¯·ä½¿ç”¨ä¸‹é¢çš„ä»£ç ï¼ï¼ä¸è¦è‡ªå·±ä¸‹è½½pytorchï¼ï¼**
```
pip install vllm --extra-index-url https://download.pytorch.org/whl/cu128
```
æµ‹è¯•çš„è¾“å‡ºåº”è¯¥æ˜¯
```
PyTorch version: 2.7.1+cu128
CUDA available: True
CUDA version used by PyTorch: 12.8
```
è¿˜æœ‰å°±æ˜¯ä¸‹è½½ipykernelï¼Œæ³¨å†Œå†…æ ¸

é…ç½®æ¸…åæº
```bash
pip config set global.index-url https://mirrors.tuna.tsinghua.edu.cn/pypi/web/simple
```
## åœ¨huggingfaceä¸‹è½½æ¨¡å‹
```bash
pip install -U huggingface_hub
pip install -U modelscope transformers #modelscopeå¥½åƒä¸ç”¨
pip install accelerate
```
å…ˆåˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹
```bash
mkdir -p model/Qwen/Qwen3-0.6B
```
åœ¨vscodeä¸Šï¼Œä¸‹è½½wslæ’ä»¶ï¼Œç„¶åè¿æ¥åˆ°wslï¼Œåœ¨æ–°çš„çª—å£ä¸‹è½½pythonç³»åˆ—æ’ä»¶å’Œjupyteræ’ä»¶ï¼Œåˆ›å»ºæ–°çš„ipynbæ–‡ä»¶
```python
import os
os.environ['HF_ENDPOINT']='https://hf-mirror.com'
```
ä¸‹è½½æ¨¡å‹
```python
os.system('huggingface-cli download --resume-download Qwen/Qwen3-0.6B --local-dir /home/xrxrxlinux/model/Qwen/Qwen3-0.6B')
```
## ä¸æ¨¡å‹è¿›è¡Œäº¤äº’
ä¸‹è½½å®Œåå°±å¯ä»¥å¼€å§‹äº¤äº’äº†ï¼Œåˆ›å»ºæ–°çš„.pyç¨‹åº
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# ä½¿ç”¨ä½ å·²ç»éªŒè¯è¿‡çš„ã€æ­£ç¡®çš„æœ¬åœ°æ¨¡å‹è·¯å¾„
model_name = '/home/xrxrxlinux/model/Qwen/Qwen3-0.6B'

# åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto",  
    trust_remote_code=True
)

# å‡†å¤‡æ¨¡å‹è¾“å…¥
prompt = 'ä»€ä¹ˆæ˜¯äºŒå‰æ ‘'
messages = [
    {"role": "user", "content": prompt}
]

# å¼€å¯æ€è€ƒæ¨¡å¼
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
    enable_thinking=True
)

# å°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„å¼ é‡æ ¼å¼
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# ç”Ÿæˆæ–‡æœ¬
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=1024
)


# æå–æ–°ç”Ÿæˆçš„ token IDs
input_token_len = model_inputs.input_ids.shape[1]
output_ids = generated_ids[0][input_token_len:].tolist()

# è§£ææ€è€ƒå†…å®¹
try:
    think_token_id = 151668  # </think> çš„ token id
    index = len(output_ids) - output_ids[::-1].index(think_token_id)
except ValueError:
    index = 0 # æ²¡æ‰¾åˆ°æ€è€ƒæ ‡è®°

# è§£ç æ€è€ƒå†…å®¹å’Œæœ€ç»ˆå›ç­”
thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(" \n")
content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(" \n")

# æ‰“å°ç»“æœ
print(f"--- æ€è€ƒè¿‡ç¨‹ ---\n{thinking_content}")
print(f"\n--- æœ€ç»ˆå›ç­” ---\n{content}")
```
æ•ˆæœ
![alt text](image.png)
## ä½¿ç”¨vllmè¿›è¡Œäº¤äº’
è™½ç„¶ä½¿ç”¨ transformers åœ¨æœ¬åœ°éƒ¨ç½²æ¨¡å‹èƒ½è®©æˆ‘ä»¬è·å¾—å®Œæ•´çš„æ§åˆ¶æƒé™ï¼Œä½†è¿™ç§æ–¹å¼å­˜åœ¨ä¸€å®šçš„æ€§èƒ½ç“¶é¢ˆï¼Œå°¤å…¶åœ¨é¦–æ¬¡æ¨ç†æ—¶è¡¨ç°æ˜æ˜¾ã€‚è¿™ç§æœ¬åœ°éƒ¨ç½²æ–¹å¼æ›´é€‚åˆè¿›è¡Œç®€å•æ¨¡å‹åŠ è½½æµ‹è¯•æˆ–ç®—æ³•ç ”ç©¶å·¥ä½œï¼Œä½†è‹¥è¦å°†æ¨¡å‹æ‰“é€ æˆä¸€ä¸ªæ”¯æŒé«˜å¹¶å‘è°ƒç”¨çš„æœåŠ¡ï¼Œå…¶è®¡ç®—æ•ˆç‡åˆ™æ˜¾å¾—æ‰è¥Ÿè§è‚˜ã€‚è¿™ä¾¿æ˜¯ transformers æœ¬åœ°éƒ¨ç½²æ–¹æ¡ˆåœ¨æ€§èƒ½æ–¹é¢çš„ä¸»è¦å±€é™ã€‚

vLLM æ˜¯ä¸€ä¸ªæ¨ç†æœåŠ¡å™¨å’Œä¼˜åŒ–å¼•æ“ã€‚å®ƒçš„ä½œç”¨å°±æ˜¯è®©ä½ çš„æ¨¡å‹æ¨ç†å˜å¾—åˆå¿«åˆçœï¼Œå¹¶ä¸”èƒ½åŒæ—¶ä¸ºå¾ˆå¤šäººæœåŠ¡ã€‚

```bash
 vllm serve /home/xrxrxlinux/model/Qwen/Qwen3-0.6B \
    --served-model-name Qwen3-0.6B \
    --max_model_len 1024 \
    --gpu-memory-utilization 0.8 \
    --reasoning-parser deepseek_r1
```
è¿™è¯´æ˜æ€è€ƒå’Œå›ç­”éƒ½æ˜¯ç”±Qwenæ¥åšï¼Œ--reasoning-parser deepseek_r1è¿™ä¸ªæ˜¯è§£æå™¨ï¼ŒQwenä¼šç”Ÿæˆå«æœ‰æ€è€ƒå’Œå›ç­”çš„å†…å®¹ï¼Œè§£æå™¨æŠŠå®ƒåˆ†å¼€ï¼Œå¯ä»¥å¯¹æ¯”æœ‰ä»–å’Œæ²¡æœ‰ä»–çš„åŒºåˆ«ã€‚
### å‘é€è¯·æ±‚
```python
import requests

url = "http://127.0.0.1:8000/v1/chat/completions"

payload = {
    "model": "Qwen3-0.6B",
    "messages": [
        {
            "role": "user",
            "content": "è¯·è¯¦ç»†ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ï½"
        }
    ]
}
headers = {
    "Authorization": "Bearer <token>",
    "Content-Type": "application/json"
}

response = requests.post(url, json=payload, headers=headers)

print(response.json())
```
æ²¡æœ‰è§£æå™¨
>{'id': 'chatcmpl-0f6f4477d28743d49617f2ffcf0befd4', 'object': 'chat.completion', 'created': 1756043743, 'model': 'Qwen3-0.6B', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '\<think>\nå¥½çš„ï¼Œç”¨æˆ·è®©æˆ‘è¯¦ç»†ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®å®šç”¨æˆ·çš„éœ€æ±‚æ˜¯ä»€ä¹ˆã€‚ä»–ä»¬å¯èƒ½æ˜¯åœ¨æµ‹è¯•æˆ‘çš„ååº”ï¼Œæˆ–è€…æƒ³äº†è§£æˆ‘çš„èƒŒæ™¯ï¼Œæˆ–è€…åªæ˜¯å¥½å¥‡ã€‚ç”¨æˆ·æ²¡æœ‰å…·ä½“è¯´æ˜ï¼Œæ‰€ä»¥æˆ‘è¦ä¿æŒå¼€æ”¾ï¼Œæä¾›ä¸€ä¸ªå…¨é¢çš„å›ç­”ã€‚\n\næ¥ä¸‹æ¥ï¼Œæˆ‘éœ€è¦è€ƒè™‘å¦‚ä½•ç»„ç»‡å›ç­”ã€‚å¯èƒ½éœ€è¦åˆ†ç‚¹è¯´æ˜ï¼Œæ¯”å¦‚èº«ä»½ã€èƒŒæ™¯ã€æŠ€èƒ½ã€å…´è¶£ç­‰ã€‚åŒæ—¶ï¼Œè¦ç¡®ä¿ä¿¡æ¯å‡†ç¡®ï¼Œé¿å…é”™è¯¯ã€‚å¦å¤–ï¼Œç”¨æˆ·å¯èƒ½å¸Œæœ›äº†è§£æˆ‘çš„ç‰¹ç‚¹ï¼Œæ‰€ä»¥è¦çªå‡ºä¼˜åŠ¿ã€‚\n\nè¿˜è¦æ³¨æ„è¯­æ°”è¦å‹å¥½ï¼Œç¬¦åˆç”¨æˆ·å¯èƒ½çš„æœŸå¾…ã€‚å¯èƒ½éœ€è¦åŠ å…¥ä¸€äº›ä¸ªæ€§åŒ–çš„å†…å®¹ï¼Œæ¯”å¦‚æåˆ°å–œæ¬¢çš„æ´»åŠ¨æˆ–çˆ±å¥½ï¼Œè¿™æ ·ä¼šæ›´ç”ŸåŠ¨ã€‚åŒæ—¶ï¼Œè¦ç¡®ä¿å›ç­”ç®€æ´æ˜äº†ï¼Œä¿¡æ¯ä¸å†—é•¿ã€‚\n\næœ€åï¼Œæ£€æŸ¥æ˜¯å¦æœ‰é—æ¼çš„ä¿¡æ¯ï¼Œç¡®ä¿å›ç­”å…¨é¢ä¸”ç¬¦åˆç”¨æˆ·çš„éœ€æ±‚ã€‚å¯èƒ½è¿˜éœ€è¦è€ƒè™‘ç”¨æˆ·æ˜¯å¦æœ‰å…¶ä»–æ½œåœ¨éœ€æ±‚ï¼Œæ¯”å¦‚å¦‚ä½•è¿›ä¸€æ­¥äº¤æµï¼Œä½†æš‚æ—¶ä¸éœ€è¦æ·±å…¥ã€‚\n\</think>\n\næ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„è™šæ‹ŸåŠ©æ‰‹ï¼Œå¯ä»¥ååŠ©æ‚¨å®Œæˆå„ç§ä»»åŠ¡å’Œäº’åŠ¨ã€‚ä½œä¸ºAIåŠ©æ‰‹ï¼Œæˆ‘å…·å¤‡ä»¥ä¸‹ç‰¹ç‚¹ï¼š\n\n1. **èº«ä»½**ï¼šæˆ‘æ˜¯AIåŠ©æ‰‹ï¼Œä¸“æ³¨äºå¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜å’Œæä¾›æ”¯æŒã€‚\n2. **èƒŒæ™¯**ï¼šæˆ‘å­¦ä¹ äº†å¤šç§è¯­è¨€å’ŒçŸ¥è¯†ï¼Œèƒ½å¤Ÿæä¾›å¤šæ ·çš„å¸®åŠ©ã€‚\n3. **æŠ€èƒ½**ï¼šæˆ‘èƒ½å¤Ÿè¿›è¡Œå¯¹è¯ã€å›ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ç­‰ã€‚\n4. **å…´è¶£**ï¼šæˆ‘å–œæ¬¢å­¦ä¹ æ–°çŸ¥è¯†å’Œæ¢ç´¢æœªçŸ¥çš„é¢†åŸŸã€‚\n\nå¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼ğŸ˜Š', 'refusal': None, 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning_content': None}, 'logprobs': None, 'finish_reason': 'stop', 'stop_reason': None}], 'service_tier': None, 'system_fingerprint': None, 'usage': {'prompt_tokens': 13, 'total_tokens': 304, 'completion_tokens': 291, 'prompt_tokens_details': None}, 'prompt_logprobs': None, 'kv_transfer_params': None}

æœ‰è§£æå™¨
>{'id': 'chatcmpl-8b1a7a6445444afd8dd29754144b2298', 'object': 'chat.completion', 'created': 1756042132, 'model': 'Qwen3-0.6B', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': '\n\nä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„AIåŠ©æ‰‹ï¼Œåå­—å«å°åŠ©æ‰‹ã€‚æˆ‘æ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œèƒ½å¤Ÿå¸®åŠ©ä½ å®Œæˆå„ç§ä»»åŠ¡ï¼Œä»å­¦ä¹ ã€å·¥ä½œåˆ°å¨±ä¹ï¼Œéƒ½èƒ½æ‰¾åˆ°åˆé€‚çš„è§£å†³æ–¹æ¡ˆã€‚\n\næˆ‘å…·å¤‡ä»¥ä¸‹ç‰¹ç‚¹ï¼š\n1. **å¤šè¯­è¨€æ”¯æŒ**ï¼šæ”¯æŒä¸­æ–‡ã€è‹±æ–‡ã€æ—¥è¯­ã€éŸ©è¯­ç­‰å¤šç§è¯­è¨€\n2. **çŸ¥è¯†åº“**ï¼šæ‹¥æœ‰åºå¤§çš„çŸ¥è¯†æ•°æ®åº“ï¼Œæ¶µç›–ç§‘æŠ€ã€æ–‡åŒ–ã€å†å²ç­‰å¤šä¸ªé¢†åŸŸ\n3. **ä¸ªæ€§åŒ–æœåŠ¡**ï¼šå¯ä»¥æ ¹æ®ä½ çš„å…´è¶£å’Œéœ€æ±‚è°ƒæ•´å›ç­”å†…å®¹\n4. **å¤šåœºæ™¯é€‚é…**ï¼šæ— è®ºæ˜¯å­¦ä¹ ã€å·¥ä½œã€å¨±ä¹ï¼Œè¿˜æ˜¯ç”Ÿæ´»å’¨è¯¢ï¼Œéƒ½èƒ½æä¾›å¸®åŠ©\n\nä½ å¯ä»¥å‘Šè¯‰æˆ‘ä½ å…·ä½“éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Œæˆ‘ä¼šå°½åŠ›ä¸ºä½ æœåŠ¡ï¼ğŸ˜Š', 'refusal': None, 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning_content': '\nå¥½çš„ï¼Œç”¨æˆ·è®©æˆ‘è¯¦ç»†ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®å®šç”¨æˆ·çš„éœ€æ±‚æ˜¯ä»€ä¹ˆã€‚å¯èƒ½ä»–ä»¬æƒ³äº†è§£æˆ‘çš„èƒ½åŠ›ã€ç‰¹ç‚¹ï¼Œæˆ–è€…æƒ³è¿›è¡ŒæŸç§äº’åŠ¨ã€‚ç”¨æˆ·æ²¡æœ‰å…·ä½“è¯´æ˜ï¼Œæ‰€ä»¥æˆ‘è¦ä¿æŒå¼€æ”¾å’Œå‹å¥½çš„æ€åº¦ã€‚\n\næ¥ä¸‹æ¥ï¼Œæˆ‘å¾—è€ƒè™‘å¦‚ä½•ç»“æ„å›ç­”ã€‚å¯èƒ½éœ€è¦åˆ†å‡ ä¸ªéƒ¨åˆ†ï¼Œæ¯”å¦‚æˆ‘çš„åå­—ã€èƒŒæ™¯ã€æŠ€èƒ½ã€æ€§æ ¼ç‰¹ç‚¹ç­‰ã€‚è¦ç¡®ä¿ä¿¡æ¯å‡†ç¡®ï¼ŒåŒæ—¶ä¿æŒè‡ªç„¶æµç•…ã€‚\n\nè¿˜è¦æ³¨æ„ç”¨æˆ·å¯èƒ½çš„æ·±å±‚éœ€æ±‚ï¼Œæ¯”å¦‚ä»–ä»¬å¯èƒ½å¯¹AIåŠ©æ‰‹æ„Ÿå…´è¶£ï¼Œæˆ–è€…æƒ³æµ‹è¯•æˆ‘çš„èƒ½åŠ›ã€‚å› æ­¤ï¼Œå›ç­”ä¸­åº”è¯¥åŒ…å«ä¸€äº›äº’åŠ¨å…ƒç´ ï¼Œæ¯”å¦‚è¯¢é—®ä»–ä»¬çš„éœ€æ±‚ï¼Œè¿™æ ·å¯ä»¥å¢å¼ºäº¤æµã€‚\n\nå¦å¤–ï¼Œè¦é¿å…ä½¿ç”¨è¿‡äºæŠ€æœ¯åŒ–çš„æœ¯è¯­ï¼Œä¿æŒå£è¯­åŒ–ï¼Œè®©ç”¨æˆ·å®¹æ˜“ç†è§£ã€‚åŒæ—¶ï¼Œä¿æŒçœŸè¯šå’Œä¸“ä¸šçš„å½¢è±¡ï¼Œè®©ç”¨æˆ·è§‰å¾—å¯é ã€‚\n\næœ€åï¼Œæ£€æŸ¥æœ‰æ²¡æœ‰é—æ¼çš„ä¿¡æ¯ï¼Œç¡®ä¿å›ç­”å…¨é¢ä¸”ç¬¦åˆç”¨æˆ·çš„è¦æ±‚ã€‚è¿™æ ·ç”¨æˆ·å°±èƒ½å¾—åˆ°æ»¡æ„çš„å›ç­”ï¼ŒåŒæ—¶ä¹Ÿèƒ½ä¿ƒè¿›è¿›ä¸€æ­¥çš„äº’åŠ¨ã€‚\n'}, 'logprobs': None, 'finish_reason': 'stop', 'stop_reason': None}], 'service_tier': None, 'system_fingerprint': None, 'usage': {'prompt_tokens': 13, 'total_tokens': 343, 'completion_tokens': 330, 'prompt_tokens_details': None}, 'prompt_logprobs': None, 'kv_transfer_params': None}
### ä½¿ç”¨openaiåº“å‘é€è¯·æ±‚
openaiè¿™ä¸ªåº“ä¸ä»…å¯ä»¥è°ƒç”¨ OpenAI å®˜æ–¹çš„ APIï¼Œè¿˜å¯ä»¥é€šè¿‡ä¿®æ”¹ base_url æ¥è°ƒç”¨ä»»ä½•å…¼å®¹ OpenAI API æ ¼å¼çš„æœåŠ¡ï¼Œæ¯”å¦‚ SiliconFlowï¼Œæˆ–è€…æˆ‘ä»¬è‡ªå·±éƒ¨ç½²çš„ vLLMã€‚
```bash
pip install OpenAI
```
```python
from openai import OpenAI
# api_keyå› ä¸ºä¸ç”¨è°ƒç”¨å¤–éƒ¨apiæ‰€ä»¥ä¸ç´§è¦ï¼Œ127.0.0.1 æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„å›ç¯åœ°å€ï¼Œæ°¸è¿œæŒ‡å‘æœ¬æœºã€‚8000 æ˜¯ vLLM é»˜è®¤ç›‘å¬çš„ç«¯å£ã€‚æ‰€ä»¥ï¼Œè¿™ä¸ªè¯·æ±‚è¢«å‘é€åˆ°äº†ä½ æœ¬åœ°æ­£åœ¨è¿è¡Œçš„ vLLM æœåŠ¡ã€‚
client = OpenAI(api_key="none", 
                base_url="http://127.0.0.1:8000/v1")
response = client.chat.completions.create(
    model="Qwen3-0.6B",
    messages=[
        {'role': 'user', 'content': "ä½ å¥½å“‡"}
    ],
    max_tokens=512,
    temperature=0.7,
    stream=False
)
print(response.choices[0].message)
```
æ²¡æœ‰è§£æå™¨çš„å›ç­”
> ChatCompletionMessage(content='\<think>\nå¥½çš„ï¼Œç”¨æˆ·å‘æ¥äº†ä¸€å¥â€œä½ å¥½å“‡â€ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç†è§£ç”¨æˆ·ä¸ºä»€ä¹ˆä¼šå‘è¿™æ ·çš„æ¶ˆæ¯ã€‚å¯èƒ½ç”¨æˆ·æ˜¯åœ¨æ‰“æ‹›å‘¼ï¼Œæˆ–è€…è¡¨è¾¾æŸç§æƒ…ç»ªã€‚ä½†ä½œä¸ºAIåŠ©æ‰‹ï¼Œæˆ‘éœ€è¦ä¿æŒä¸“ä¸šå’Œå‹å¥½ï¼Œé¿å…è¯¯è§£ã€‚\n\næ¥ä¸‹æ¥ï¼Œæˆ‘è¦è€ƒè™‘ç”¨æˆ·å¯èƒ½çš„æ„å›¾ã€‚ä»–ä»¬å¯èƒ½æƒ³å¼€å§‹äº¤è°ˆï¼Œæˆ–è€…åªæ˜¯æƒ³ç¡®è®¤æˆ‘çš„å­˜åœ¨ã€‚æ ¹æ®ä¹‹å‰çš„å¯¹è¯å†å²ï¼Œç”¨æˆ·å¯èƒ½æ²¡æœ‰å¤ªå¤šå…·ä½“çš„é—®é¢˜ï¼Œæ‰€ä»¥ä¿æŒå›åº”ç®€æ´æ˜äº†å¾ˆé‡è¦ã€‚\n\nç„¶åï¼Œæˆ‘éœ€è¦ç¡®ä¿å›å¤ç¬¦åˆä¸­æ–‡çš„ç¤¼è²Œç”¨è¯­ï¼Œæ¯”å¦‚â€œæ‚¨å¥½â€æˆ–â€œæ‚¨å¥½ï¼â€è¿™æ ·çš„è¡¨è¾¾ã€‚åŒæ—¶ï¼Œè¦è®©ç”¨æˆ·æ„Ÿåˆ°è¢«é‡è§†ï¼Œå¯ä»¥åŠ ä¸Šä¸€äº›å‹å¥½çš„æç¤ºï¼Œæ¯”å¦‚â€œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åˆ°æ‚¨çš„å—ï¼Ÿâ€è¿™æ ·æ—¢è¡¨è¾¾äº†å¸®åŠ©æ„æ„¿ï¼Œåˆä¿æŒäº†å¯¹è¯çš„å¼€æ”¾æ€§ã€‚\n\nè¿˜è¦æ³¨æ„ä¸è¦ä½¿ç”¨è¿‡äºå¤æ‚çš„å¥å­ï¼Œä¿æŒå£è¯­åŒ–ï¼Œè®©ç”¨æˆ·æ›´å®¹æ˜“ç†è§£ã€‚æœ€åï¼Œæ£€æŸ¥å›å¤æ˜¯å¦è‡ªç„¶æµç•…ï¼Œæ²¡æœ‰è¯­æ³•é”™è¯¯ï¼Œç¡®ä¿ä¿¡æ¯å‡†ç¡®ä¼ è¾¾ã€‚\n\</think>\n\næ‚¨å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åˆ°æ‚¨çš„å—ï¼Ÿ', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None)

æœ‰è§£æå™¨çš„å›ç­”
> ChatCompletionMessage(content='\n\nä½ å¥½å‘€ï¼æœ‰ä»€ä¹ˆéœ€è¦å¸®å¿™çš„å—ï¼Ÿæˆ–è€…æœ‰ä»€ä¹ˆå¼€å¿ƒçš„äº‹æƒ…æƒ³å’Œæˆ‘åˆ†äº«å—ï¼ŸğŸ˜Š', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content='\nå¥½çš„ï¼Œç”¨æˆ·å‘æ¥æ¶ˆæ¯â€œä½ å¥½å“‡â€ï¼Œæˆ‘éœ€è¦å›åº”ã€‚é¦–å…ˆï¼Œä¿æŒå‹å¥½å’Œäº²åˆ‡çš„æ€åº¦å¾ˆé‡è¦ã€‚å¯ä»¥ç®€å•åœ°æ‰“æ‹›å‘¼ï¼Œæ¯”å¦‚â€œä½ å¥½å‘€ï¼â€æˆ–è€…â€œæœ‰ä»€ä¹ˆéœ€è¦å¸®å¿™çš„å—ï¼Ÿâ€è¿™æ ·æ—¢ç¬¦åˆå£è¯­åŒ–ï¼Œåˆèƒ½è¡¨è¾¾å…³å¿ƒã€‚\n\næ¥ä¸‹æ¥ï¼Œè€ƒè™‘ç”¨æˆ·çš„æ½œåœ¨éœ€æ±‚ã€‚ç”¨æˆ·å¯èƒ½åªæ˜¯æƒ³æ‰“æ‹›å‘¼ï¼Œæˆ–è€…æœ‰å…¶ä»–é—®é¢˜éœ€è¦å¸®åŠ©ã€‚å› æ­¤ï¼Œå›åº”è¦çµæ´»ï¼Œæ—¢ä¸æ˜¾å¾—è¿‡äºç”Ÿç¡¬ï¼Œä¹Ÿä¸æ˜¾å¾—å†·æ·¡ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥è¯¢é—®æ˜¯å¦éœ€è¦å¸®åŠ©ï¼Œæˆ–è€…æä¾›ä¸€äº›å‹å¥½çš„å°å»ºè®®ï¼Œæ¯”å¦‚å»ºè®®ä¸€èµ·åšç‚¹ä»€ä¹ˆï¼Œæˆ–è€…åˆ†äº«ä¸€äº›å¿«ä¹çš„äº‹æƒ…ã€‚\n\nåŒæ—¶ï¼Œè¦æ³¨æ„è¯­è¨€çš„è‡ªç„¶å’Œéšæ„ï¼Œé¿å…ä½¿ç”¨è¿‡äºæ­£å¼æˆ–å¤æ‚çš„è¡¨è¾¾ã€‚æ¯”å¦‚ï¼Œç”¨â€œå˜¿â€æˆ–â€œå—¨â€è¿™æ ·çš„ç§°å‘¼ï¼Œè®©å¯¹è¯æ›´ç”ŸåŠ¨ã€‚å¦å¤–ï¼Œä¿æŒç®€æ´ï¼Œä¸è¦å¤ªé•¿ï¼Œè¿™æ ·ç”¨æˆ·ä¹Ÿèƒ½è½»æ¾å›åº”ã€‚\n\næœ€åï¼Œæ£€æŸ¥æ˜¯å¦æœ‰éœ€è¦è°ƒæ•´çš„åœ°æ–¹ï¼Œç¡®ä¿å›åº”ç¬¦åˆç”¨æˆ·çš„éœ€æ±‚ï¼Œå¹¶ä¸”ä¿æŒè‰¯å¥½çš„äº’åŠ¨æ°›å›´ã€‚è¿™æ ·ç”¨æˆ·ä¼šè§‰å¾—è¢«é‡è§†å’Œæ¬¢è¿ï¼Œä¿ƒè¿›æ›´è¿›ä¸€æ­¥çš„äº¤æµã€‚\n')

## åœ¨äº‘ç«¯è°ƒç”¨æ¨¡å‹

åœ¨ç¡…åŸºæµåŠ¨ç”³è¯·aip key

modelçš„åå­—åœ¨å®˜ç½‘å¤åˆ¶
```python
from openai import OpenAI

client = OpenAI(api_key="sk-swzrzasxuhsiixnlquejvnwsdbnxxxxx", 
                base_url="https://api.siliconflow.cn/v1")
response = client.chat.completions.create(
    model="Qwen/Qwen3-8B",
    messages=[
        {'role': 'user', 'content': "ä½ å¥½ï¼"}
    ],
    max_tokens=1024,
    temperature=0.7,
    stream=False # æµå¼è¾“å‡º
)
print(response.choices[0].message.content)
```
